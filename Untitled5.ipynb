{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNURpMSibR1xAYt66OYcczC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pasn2/NeuronNetworks/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Iv5RwlYPzEkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0537157-aa50-44ee-e5fa-5ed4f0d1c5ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1563/1563 - 44s - loss: 1.6616 - accuracy: 0.4014 - 44s/epoch - 28ms/step\n",
            "Epoch 2/5\n",
            "1563/1563 - 38s - loss: 1.3892 - accuracy: 0.5089 - 38s/epoch - 24ms/step\n",
            "Epoch 3/5\n",
            "1563/1563 - 36s - loss: 1.2513 - accuracy: 0.5612 - 36s/epoch - 23ms/step\n",
            "Epoch 4/5\n",
            "1563/1563 - 37s - loss: 1.1553 - accuracy: 0.5954 - 37s/epoch - 23ms/step\n",
            "Epoch 5/5\n",
            "1563/1563 - 37s - loss: 1.0988 - accuracy: 0.6147 - 37s/epoch - 24ms/step\n",
            "313/313 [==============================] - 5s 14ms/step - loss: 1.1149 - accuracy: 0.6128\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.114856243133545, 0.6128000020980835]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "(d_train,d_test),d_info = tfds.load(\n",
        "    \"cifar10\",\n",
        "    split=[\"train\",\"test\"],\n",
        "    shuffle_files = True,\n",
        "    as_supervised=True,\n",
        "    with_info=True,\n",
        ")\n",
        "def normalize_img(image,label):\n",
        "  return tf.cast(image,tf.float32) /255.0,label\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "def augment(image,label):\n",
        "  new_height = new_width = 32\n",
        "  image = tf.image.resize(image,(new_height,new_width))\n",
        "\n",
        "  if tf.random.uniform((),minval=0,maxval=1) < 0.1:\n",
        "    image = tf.tile(tf.image.rgb_to_grayscale(image),[1,1,3])\n",
        "\n",
        "    \n",
        "d_train = d_train.map(normalize_img,num_parallel_calls=AUTOTUNE)\n",
        "d_train = d_train.cache()\n",
        "d_train = d_train.shuffle(d_info.splits[\"train\"].num_examples)\n",
        "d_train = d_train.batch(BATCH_SIZE)\n",
        "d_train = d_train.prefetch(AUTOTUNE)\n",
        "\n",
        "d_test = d_test.map(normalize_img,num_parallel_calls=AUTOTUNE)\n",
        "d_test = d_test.batch(BATCH_SIZE)\n",
        "d_test = d_test.prefetch(AUTOTUNE)\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.Input((32,32,3)),\n",
        "        layers.Conv2D(4,3,padding=\"same\",activation=\"relu\"),\n",
        "        layers.Conv2D(8,3,padding=\"same\",activation=\"relu\"),\n",
        "        layers.MaxPooling2D(),\n",
        "        layers.Conv2D(16,3,padding=\"same\",activation=\"relu\"),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64,activation=\"relu\"),\n",
        "        layers.Dense(10),\n",
        "    ]\n",
        ")\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(3e-4),\n",
        "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(d_train,epochs=5,verbose=2)\n",
        "model.evaluate(d_test)"
      ]
    }
  ]
}